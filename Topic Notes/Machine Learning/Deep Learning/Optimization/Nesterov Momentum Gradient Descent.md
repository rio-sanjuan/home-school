The key idea behind Nesterov Momentum is that instead of using only the gradient at the location where we currently are, we also use the gradient at the location where we expect that we're going to be. Because we can't really predict the future, we estimate where we're going to be on the next step and use the gradient there. The thinking is that if the error surface is relatively smooth, and our estimate is pretty good, then the gradient we find at our estimated next position is close to the gradient where we'd actually end up if we just moved using the standard gradient descent, with or without momentum.

Any time we use [[Momentum Gradient Descent]], it's worth considering Nesterov Momentum instead. It requires no additional parameters from us, but it usually learns more quickly and with less noise.