Instead of summing up all the squared gradients since the beginning of training, suppose we keep a *decaying sum* of these gradients. We can think of this as a running list of the most recent gradients for each weight. Each time we update the weights, we tack the new gradient onto the end of the list and drop the oldest one off the start. To find the value we use to divide the new gradient, we add up all the values in the list, but we first multiply them all by a number based on their position in the list. Recent values get multiplied by a larger value, while the oldest ones get multiplied by a very small value. This way our running sum is most heavily determined by recent gradients, though it is influenced to a lesser degree by the older gradients. In this way, the running sum of the gradients (and thus the value we divided new gradients by) can go up and down based on the gradients we've applied recently. This algorithm is called *Adadelta*.

Since Adadelta adjusts the learning rates on the weights individually, any weight that's been on a steep slope for a while will slow down so it doesn't go flying off, but when that weight is on a flatter section, it's allowed to take bigger steps. Like [[Adagrad]], we often start the learning rate at a value around 0.01, and then let the algorithm adjust it from then on.

Adadelta has the downside of requiring another parameter, which is also called $\gamma$. It's roughly related to the parameter $\gamma$ used by [[Momentum Gradient Descent]]-type algorithms, but they are different. The value of $\gamma$ here tells us how much we scale down the gradients in our history list over time. A large value of $\gamma$ "remembers" values from farther back than smaller values and will let them contribute to the sum. A smaller value of $\gamma$ just focuses on recent gradients. Often we set this $\gamma$ to around 0.9.

There's another parameter in Adadelta called $\epsilon$ that's used to keep the calculations numerically stable. Most libraries will set this to a default value that's carefully selected by the programmers to make things work as well as possible, so it should never be changed unless there's a specific need.