Adaptive Gradient Learning (Adagrad) adapts the size of the gradient for each weight instead of assume a constant learning rate $\eta$ for all weights. Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis. For each weight, Adagrad takes the gradient that we use in that update step, squares it, and adds that into a running sum for that weight. Then the gradient is divided by a value derived from this sum, giving us the value that's then used for the update. Because each step's gradient is squared before it's added in, the value that's added into the sum is always positive. As a result, this running sum gets larger and larger over time. To keep it form growing out of control, we divide each change by that growing sum, so the changes to each weight get smaller and smaller over time.

This sounds a lot like learning rate decay. As time goes on, the changes to the weights get smaller. The difference here is that the slowdown in learning is being computed uniquely for each weight based on its history. Because Adagrad is effectively automatically computing a learning rate for every weight on the fly, the learning rate we use to kick things off isn't as critical as it was for earlier algorithms. This is a huge benefit, since it frees us from the task of fine-tuning that error rate. We often set the learning rate $\eta$ to a small value like 0.01 and let Adagrad handle things from there.

Because the sum of the gradients get larger over time, eventually we'll find that dividing each new gradient by a value related to that sum gives us gradients that approach 0. The increasingly small updates are why the error curve for Adagrad descends so very slowly as it tries to get rid of that last remaining error.