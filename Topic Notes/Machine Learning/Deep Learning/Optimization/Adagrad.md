Adaptive Gradient Learning (Adagrad) adapts the size of the gradient for each weight instead of assume a constant learning rate $\eta$ for all weights. Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis. For each weight, Adagrad takes the gradient that we use in that update step, squares it, and adds that into a running sum for that weight. Then the gradient is divided by a value derived from this sum, giving us the value that's then used for the update. Because each step's gradient is squared before it's added in, the value that's added into the sum is always positive. As a result, this running sum gets larger and larger over time. To keep it form growing out of control, we divide each change by that growing sum, so the changes to each weight get smaller and smaller over time.

This sounds a lot like learning rate decay. As time goes on, the changes to the weights get smaller. The difference here is that the slowdown in learning is being computed uniquely for each weight based on its history. Because Adagrad is effectively automatically computing a learning rate for every wei