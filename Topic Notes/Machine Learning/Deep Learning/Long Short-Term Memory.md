
The overall structure of the LSTM is the same as the traditional [[Recurrent Neural Network]] model. It also has a chain structure that has identical neural network blocks applying to the elements of the sequence. The key difference is that a set of gating units is utilized to control the information flow in LSTM. The information flowing through consecutive positions in a sequence includes the cell state $C^{(t-1)}$ and the hidden state $h^{(t-1)}$. The cell state serves as the information from the previous states that are propagated to the next position and the hidden state helps decide how the information should be propagated. The hidden state $h^{(t)}$ also serves as the output of this position if necessary.

LSTMs can effectively decide when to keep or discard information, enabling them to model long-term dependencies. The gating mechanisms also mitigate the [[Vanishing-Exploding Gradient]] issue, making LSTMs more stable during training. They work well with variable-length sequences and are widely used in natural language processing, time-series analysis, and speech processing. Traditional RNNs update the hidden state directly using only the current input and the previous hidden state, leading to difficulty in retaining long-term dependencies. LSTMs address this by explicitly modeling memory with the cell state and gates, making them more robust for complex, long-sequence tasks.