Formula: $$ \text{swish}_\beta(x) = x\;\text{sigmoid}(ğ›½ x) = \frac{x}{1 + e^{-ğ›½ x}}$$
The swish family of functions was designed to smoothly interpolate between a linear function and the ReLU function. Often defined $ğ›½ = 1$, the swish function is smooth and non-monotonic, which can help the model with gradient flow and reduce the likelihood of vanishing gradients. It tends to outperform ReLU in some deep network architectures.