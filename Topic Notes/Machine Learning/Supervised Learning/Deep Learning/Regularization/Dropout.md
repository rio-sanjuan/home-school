A popular [[Regularization]] method is called *dropout*. It is usually applied to a deep network in the form of a *dropout layer*. The dropout layer is called an *accessory layer* or *supplemental layer*, because it doesn't do any computation of its own. We call it a layer, and draw it as one, because it's convenient conceptually, and lets us include dropout in drawings of networks, but we don't consider it a real layer (hidden or otherwise).

Dropout is a placeholder that tells the network to run an algorithm on the previous layer, and it's only active during training. The job of the dropout layer is to temporarily disconnect some of the neurons on the previous layer. We give it a parameter that describes the percentage of neurons that should be affected, and at the start of each batch, it randomly chooses that percentage of neurons on the preceding layer and temporarily disconnects their inputs and outputs from the network. Since they're disconnected, these neurons don't participate in any forward calculations, and they're not included in [[Backpropagation]], and the weights coming into them are not updated by the optimizer. When the batch is done and the rest of the weights have been updated, the chosen neurons and all of their connections are restored.

Dropout delays overfitting by preventing any neurons from overspecializing and dominating. Suppose that one neuron in a photo classification system gets highly specialized to detect the eyes of cats. That's useful for recognizing pictures of cats' faces, but useless for all the other photographs the system might be asked to classify. Dropout helps us avoid this kind of specialization. When a neuron is disconnected, the remaining neurons must adjust to pick up the slack. Thus, the specialized neuron is freed up to perform a more generally useful task, and we've delayed the onset of overfitting. Dropout helps us put off overfitting by spreading around the learning among all the neurons.