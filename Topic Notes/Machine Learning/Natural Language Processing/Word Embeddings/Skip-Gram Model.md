The Skip-Gram Model is a neural network-based technique used to learn word embeddings, which are dense vector representations of words in a continuous vector space. Introduced in the [[Word2Vec]] framework by #Mikolov et al., the algorithm aims to predict the context words surrounding a target word within a fixed window in a sentence or document. By doing this, it captures semantic and syntactic relationships between words based on their co-occurrence patterns.

In the skip-gram model, the input is a single word (the target), and the network is trained to predict its neighboring words (the context). The algorithm maximizes the probability of correctly predicting context words given the target word. This is done by feeding the input word into an embedding layer that maps it to a vector, which is then used to estimate probabilities over the vocabulary for each context word. 

The skip-gram model is particularly effective at capturing complex relationships in text, such as word similarity and analogies. It is computationally efficient and can scale to large datasets through optimizations like [[Negative Sampling]] and [[Hierarchical Softmax]]. Beyond NLP, the skip-gram approach has inspired similar techniques in graph embedding methods like [[Embedding#DeepWalk]] and [[Embedding#Node2Vec]], where nodes in a graph are treated analogously to words in a sentence.